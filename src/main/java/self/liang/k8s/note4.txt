1.部署
    强行二进制部署，不二进制，不理解
        https://blog.csdn.net/fengfeng0328/article/details/85195695
2.准备
    5台 2c/4g/50g 使用同一内网
    centos7.6
    bind9
    自签名证书环境
    docker 以及harbor

3.调整yum源
 安装epel-release   (EPEL (Extra Packages for Enterprise Linux)是基于Fedora的一个项目，为“红帽系”的操作系统提供额外的软件包，适用于RHEL、CentOS和Scientific Linux.通过epel源可以安装大部分的软件包，)
    yum install epel-release



4.dns 服务初始化  (详情看dns相关课程)
    1.安装bind9 yum install bind -y
    2.配置bind9
        1. /etc/named.conf
            修改
            listen on prot 53{ 内网地址 }
            allow-query {any;}
            dnssec-enable:no;
            dnssec-validation no;
            添加：
            forwarders  { 内网网关 }

    3 named-checkconf
       没有报错则表明 配置named.conf 没有配置错误

    4.配置区域配置文件  (课程规划了两个域，主机域以及业务域)
    /etc/named.rfc1912.zones
    zone "host.com" IN{
        type master;
        file "host.com.zone";
        allow-update{172.26.216.148;};
    };

    zone "od.com" IN{
        type master;
        file "od.com.zone";
        allow-update{172.26.216.148;};
    }

    5 配置主机域数据文件：
       /var/named/host.com.zone

        $ORIGIN  host.com.
        $TTL 600        ;10 minutes
        @ IN SOA dns.host.com. dnsadmin.host.com. (
                2020121215;Serial
                10800;Refresh
                900;Retry
                604800;Expire
                86400;Minimum
        )
         NS dns.host.com.
        $TTL 60; 1 minute
        dns             A       172.26.216.148
        HD-148          A       172.26.216.148
        HD-145          A       172.26.216.145
        HD-147          A       172.26.216.147
        HD-149          A       172.26.216.149
        HD-146          A       172.26.216.146



     /var/named/od.com.zone


    $ORIGIN  od.com.
    $TTL 600        ;10 minutes
    @ IN SOA dns.od.com. dnsadmin.od.com. (
            2020121215;Serial
            10800;Refresh
            900;Retry
            604800;Expire
            86400;Minimum
    )
     NS dns.od.com.
    $TTL 60; 1 minute
    dns             A       172.26.216.148

    named-checkconf 检查一下
    naemd-chekczone
    systemctl restart named

    yum install bind-utils
    dig -t A hd-149.host.com @172.26.216.148 +short  得到解析的地址

    6. 其他主机设置
    vim /etc/sysconfig/network-scripts/ifcfg-eth0
    systemctl restart network
      DNS1：172.26.216.148

      之后cat /etc/resolv.conf
        options timeout:2 attempts:3 rotate single-request-reopen
        ; generated by /usr/sbin/dhclient-script
        nameserver 172.26.216.148

      修改resolv.conf 添加
      search host.com
      (短域名搜索)


5.准备签发证书
    1.安装CFSSL

wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
chmod +x cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x cfssljson_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl-certinfo_linux-amd64
mkdir -p /root/local/bin
mv cfssl_linux-amd64 /root/local/bin/cfssl
mv cfssl-certinfo_linux-amd64 /root/local/bin/cfssl-certinfo
mv cfssljson_linux-amd64 /root/local/bin/cfssljson
export PATH=/root/local/bin:$PATH
export PATH=/home/liang/local/bin:$PATH

    2.需要一个根证书. 创建生产ca证书签名请求（csr)的json配置文件
    ca-csr.json 文件
    {
        "CN": "phantom5702",
        "hosts": [],
        "key": {
            "algo": "rsa",
            "size": 2048
        },
        "names": [{
            "C": "CN",
            "L": "beijing",
            "ST": "beijing",
            "O": "phantom5702",
            "OU": "ops"
        }],
        "ca": {
            "expiry": "175200h"
        }
    }
    cfssl gencert -initca ca-csr.json | cfssljson -bare ca
    [root@iZ8vb7b85wvb10l0kcmk03Z certs]# ls
    ca.csr  ca-csr.json  ca-key.pem  ca.pem

    CN: Common Name，浏览器使用该字段验证网站是否合法，一般写的是域名。非常重要。浏览器使用该字段验证网站是否合法
    C: Country， 国家
    ST: State，州，省
    L: Locality，地区，城市
    O: Organization Name，组织名称，公司名称
    OU: Organization Unit Name，组织单位名称，公司部门

6 部署docker环境
     部署三台机器
    curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun
    /etc/docker/daemon.json
{
     "graph":"/var/lib/docker",
     "storage-driver":"overlay2",
     "insecure-registries": ["registry.access.redhat.com","quay.io","harbor.phantom5702.com"],
     "registry-mirrors":["https://tuj12vuh.mirror.aliyuncs.com"],
     "bip": "172.7.#{自己的ip最后尾数}.1/24",
     "exec-opts": ["native.cgroupdriver=systemd"],
     "live-restore":true
}

cat >/etc/docker/daemon.json <<EOF
{
  "graph": "/data/docker",
  "storage-driver": "overlay2",
  "insecure-registries": ["registry.access.redhat.com","quay.io","harbor.phantom5702.com"],
  "registry-mirrors": ["https://tuj12vuh.mirror.aliyuncs.com"],
  "bip": "172.7.161.1/24",
  "exec-opts": ["native.cgroupdriver=systemd"],
  "live-restore": true
}
EOF

7.docker 私有仓库
    1.下载harbor
       git获取并解压
       编辑harbor.yml
       修改：hostname,修改为自己的工作域
            port 180
            harbor_admin_password  不改了
            log 地址
            data_volume

    2.依赖docker-compose
        yum install docker-compose -y

    3.调用install.sh   这里吧harbor.yml 的https相关配置注了，不然需要配置证书

    4.在此机器上安装nginx
        添加配置文件在nginx.conf下：  harbor.od.com.conf
server {
    listen       80;
    server_name  harbor.od.com;
    client_max_body_size 1000m;
    location / {
            proxy_pass http://127.0.0.1:180;
    }

}

server {
        listen       80;
        server_name  #{ip};
        client_max_body_size 1000m;
        location / {
                proxy_pass http://127.0.0.1:180;
        }
}

        systemctl start nginx
        systemctl enable nginx

    5.修改dns配置 /var/named/od.com.zone

        $ORIGIN  od.com.
        $TTL 600        ;10 minutes
        @ IN SOA dns.od.com. dnsadmin.od.com. (
                #注意序号要变动
                2020121216;Serial
                10800;Refresh
                900;Retry
                604800;Expire
                86400;Minimum
        )
         NS dns.od.com.
        $TTL 60; 1 minute
        dns             A       172.26.216.148
         #添加解析
        harbo           A       172.26.216.147
        systemctl restart named

    dig -t A harbor.od.com +short

    6.docker pull nginx:v1.7.9

        docker tag 7baf28ea91eb harbor.od.com/public/nginx:1.7.9 打一个tag
        docker push harbor.phantom5702.com/public/nginx:1.7.9  这里会失败

        docker login harbor.od.com
        之后可以push成功

   7. 提前准备pauser/nginx基础镜像

        pauser镜像是k8s启动pod时,预先用来创建相关资源(如名称空间)的
        nginx镜像是k8s部署好以后,我们测试pod创建所用的

        docker pull kubernetes/pause
        docker tag kubernetes/pause:latest harbor.phantom5702.com/public/pause:latest
        docker push harbor.phantom5702.com/public/pause:latest


        docker login harbor.zq.com -uadmin -pHarbor12345
        docker pull kubernetes/pause
        docker pull nginx:1.17.9
        docker tag kubernetes/pause:latest harbor.zq.com/public/pause:latest
        docker tag nginx:1.17.9 harbor.zq.com/public/nginx:v1.17.9
        docker push harbor.zq.com/public/pause:latest
        docker push harbor.zq.com/public/nginx:v1.17.9

8.安装master的etcd  (test2,test3,test4)  3台机器
    1.给etcd签发证书，etcd之间通信需要证书
      创建基于根证书的config配置文件 ca-config.json
        {
            "signing": {
                "default": {
                    "expiry": "175200h"
                },
                "profiles": {
                    "server": {
                        "expiry": "175200h",
                        "usages": [
                            "signing",
                            "key encipherment",
                            "server auth"
                        ]
                    },
                    "client": {
                        "expiry": "175200h",
                        "usages": [
                            "signing",
                            "key encipherment",
                            "client auth"
                        ]
                    },
                    "peer": {
                        "expiry": "175200h",
                        "usages": [
                            "signing",
                            "key encipherment",
                            "server auth",
                            "client auth"
                        ]
                    }
                }
            }
        }

        证书时间统一为10年,不怕过期
        证书类型
        client certificate：客户端使用，用于服务端认证客户端,例如etcdctl、etcd proxy、fleetctl、docker客户端
        server certificate：服务端使用，客户端以此验证服务端身份,例如docker服务端、kube-apiserver
        peer certificate：双向证书，用于etcd集群成员间通信

    创建etcd请求的证书文件
        注意:
        需要将所有可能用来部署etcd的机器,都加入到hosts列表中
        否则后期重新加入不在列表中的机器,需要更换所有etcd服务的证书
    {
        "CN": "k8s-etcd",
        "hosts": [
          "172.26.216.164",
          "172.26.216.163",
          "172.26.216.162",
          "172.26.216.161",
          "172.26.216.142"
        ],
        "key": {
          "algo": "rsa",
          "size": 2048
        },
        "names": [
          {
            "C": "CN",
            "ST": "BeiJing",
            "L": "BeiJing",
            "O": "od",
            "OU": "ops"
          }
        ]
    }

    cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer etcd-peer-csr.json

    cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer etcd-peer-csr.json | cfssljson -bare etcd-peer

    2. 创建etcd用户
        useradd -s /sbin/nologin -M etcd
    3.下载etcd github
         tar xf etcd-v3.3.10-linux-amd64.tar.gz -C /opt/
        cd /opt/
        mv etcd-v3.3.10-linux-amd64/ etcd-v3.3.10
        ln -s /opt/etcd-v3.3.10/ /opt/etcd

    4.获取生成的证书文件
        ca.pem   etcd-peer-key.pem  etcd-peer.pem



        创建目录，拷贝证书文件
        创建证书目录、数据目录、日志目录
        mkdir -p /opt/etcd/certs /data/etcd /data/logs/etcd-server
        chown -R etcd.etcd /opt/etcd-v3.3.10/
        chown -R etcd.etcd /data/etcd/
        chown -R etcd.etcd /data/logs/etcd-server/
        拷贝生成的证书文件
        cd /opt/etcd/certs
        scp www.phantom5702.com:/opt/certs/ca.pem .
        scp www.phantom5702.com:/opt/certs/etcd-peer.pem .
        scp www.phantom5702.com:/opt/certs/etcd-peer-key.pem .
        chown -R etcd.etcd /opt/etcd/certs
        也可以先创建一个NFS,直接从NFS中拷贝

      mv /root/*.pem /opt/etcd/certs
    5.创建执行脚本etcd  参数根据情况改变

    tar xf etcd-v3.3.10-linux-amd64.tar.gz -C /opt/
    cd /opt/
    mv etcd-v3.3.10-linux-amd64/ etcd-v3.3.10
    ln -s /opt/etcd-v3.3.10/ /opt/etcd

cat >/opt/etcd/etcd-server-startup.sh <<'EOF'
#!/bin/sh
./etcd \
    --name etcd-server-162 \
    --data-dir /data/etcd/etcd-server \
    --listen-peer-urls https://172.26.216.163:2380 \
    --listen-client-urls https://172.26.216.163:2379,http://127.0.0.1:2379 \
    --quota-backend-bytes 8000000000 \
    --initial-advertise-peer-urls https://172.26.216.163:2380 \
    --advertise-client-urls https://172.26.216.163:2379,http://127.0.0.1:2379 \
    --initial-cluster  etcd-server-164=https://172.26.216.164:2380,etcd-server-163=https://172.26.216.163:2380,etcd-server-161=https://172.26.216.161:2380 \
    --ca-file ./certs/ca.pem \
    --cert-file ./certs/etcd-peer.pem \
    --key-file ./certs/etcd-peer-key.pem \
    --client-cert-auth  \
    --trusted-ca-file ./certs/ca.pem \
    --peer-ca-file ./certs/ca.pem \
    --peer-cert-file ./certs/etcd-peer.pem \
    --peer-key-file ./certs/etcd-peer-key.pem \
    --peer-client-cert-auth \
    --peer-trusted-ca-file ./certs/ca.pem \
    --log-output stdout
EOF


    6.改变etcd的所属人和组
       [root@iZ8vb7b85wvb10l0kcmk03Z opt]# chown -R etcd.etcd /opt/etcd-bin
       [root@iZ8vb7b85wvb10l0kcmk03Z opt]# chown -R etcd.etcd /data/etcd
       [root@iZ8vb7b85wvb10l0kcmk03Z opt]# chown -R etcd.etcd /data/logs/etcd-server/

    7 可选 （考虑用nohup 简单化）
      supervisor

      nohup sh ./etcd-server-startup.sh >> test.log 2>&1 &

      netstat -luntp | grep etcd
      看到监听 2379 和 2380
    启动失败看   https://blog.csdn.net/qdqht2009/article/details/87937941
    https://blog.csdn.net/love8753/article/details/88972918

    rm -rf /data/etcd/etcd-server/member/

9.kube-apiServer 集群
    计划放在 两个slave上
    在github上下载 解压

    如果etcd配置了证书，则这里需要配置证书

    9.1 签发client端证书
    此证书的用途是apiserver和etcd之间通信所用

    cat >/opt/certs/client-csr.json <<EOF
    {
        "CN": "k8s-node",
        "hosts": [
        ],
        "key": {
            "algo": "rsa",
            "size": 2048
        },
        "names": [
            {
                "C": "CN",
                "ST": "beijing",
                "L": "beijing",
                "O": "zq",
                "OU": "ops"
            }
        ]
    }
    EOF

    cfssl gencert \
          -ca=ca.pem \
          -ca-key=ca-key.pem \
          -config=ca-config.json \
          -profile=client \
          client-csr.json |cfssljson -bare client


    9.2 签发kube-apiserver证书
    此证书的用途是apiserver对外提供的服务的证书
    此配置中的hosts包含所有可能会部署apiserver的列表
    其中10.4.7.10是反向代理的vip地址
    cat >/opt/certs/apiserver-csr.json <<EOF
    {
        "CN": "k8s-apiserver",
        "hosts": [
            "127.0.0.1",
            "192.168.0.1",
            "kubernetes.default",
            "kubernetes.default.svc",
            "kubernetes.default.svc.cluster",
            "kubernetes.default.svc.cluster.local",
            "server.phantom5702.com",
            "172.26.216.164",
            "172.26.216.163",
            "172.26.216.162",
            "172.26.216.161",
            "172.26.216.142"
        ],
        "key": {
            "algo": "rsa",
            "size": 2048
        },
        "names": [
            {
                "C": "CN",
                "ST": "beijing",
                "L": "beijing",
                "O": "zq",
                "OU": "ops"
            }
        ]
    }
    EOF

    cfssl gencert \
          -ca=ca.pem \
          -ca-key=ca-key.pem \
          -config=ca-config.json \
          -profile=server \
          apiserver-csr.json |cfssljson -bare apiserver

    使用脚本启动

    9.3下载安装kube-apiserver
    # 上传并解压缩
    tar xf kubernetes-server-linux-amd64.tar.gz  -C /opt
    cd /opt
    mv kubernetes/ kubernetes-v1.15.2
    ln -s /opt/kubernetes-v1.15.2/ /opt/kubernetes
    cd /opt/kubernetes
    rm -rf kubernetes-src.tar.gz
    cd server/bin
    rm -f *.tar
    rm -f *_tag
    ln -s /opt/kubernetes/server/bin/kubectl /usr/bin/kubectl

    部署apiserver服务
    4.4.1 拷贝证书文件
    mkdir -p /opt/kubernetes/server/bin/cert
    cd /opt/kubernetes/server/bin/cert
    scp hdss7-200:/opt/certs/ca.pem .
    scp hdss7-200:/opt/certs/ca-key.pem .
    scp hdss7-200:/opt/certs/client.pem .
    scp hdss7-200:/opt/certs/client-key.pem .
    scp hdss7-200:/opt/certs/apiserver.pem .
    scp hdss7-200:/opt/certs/apiserver-key.pem .

    4.4.2创建audit配置
    audit日志审计规则配置是k8s要求必须要有得配置,可以不理解,直接用
    mkdir /opt/kubernetes/server/conf
    cat >/opt/kubernetes/server/conf/audit.yaml <<'EOF'
    apiVersion: audit.k8s.io/v1beta1 # This is required.
    kind: Policy
    # Don't generate audit events for all requests in RequestReceived stage.
    omitStages:
      - "RequestReceived"
    rules:
      # Log pod changes at RequestResponse level
      - level: RequestResponse
        resources:
        - group: ""
          # Resource "pods" doesn't match requests to any subresource of pods,
          # which is consistent with the RBAC policy.
          resources: ["pods"]
      # Log "pods/log", "pods/status" at Metadata level
      - level: Metadata
        resources:
        - group: ""
          resources: ["pods/log", "pods/status"]
      # Don't log requests to a configmap called "controller-leader"
      - level: None
        resources:
        - group: ""
          resources: ["configmaps"]
          resourceNames: ["controller-leader"]
      # Don't log watch requests by the "system:kube-proxy" on endpoints or services
      - level: None
        users: ["system:kube-proxy"]
        verbs: ["watch"]
        resources:
        - group: "" # core API group
          resources: ["endpoints", "services"]
      # Don't log authenticated requests to certain non-resource URL paths.
      - level: None
        userGroups: ["system:authenticated"]
        nonResourceURLs:
        - "/api*" # Wildcard matching.
        - "/version"
      # Log the request body of configmap changes in kube-system.
      - level: Request
        resources:
        - group: "" # core API group
          resources: ["configmaps"]
        # This rule only applies to resources in the "kube-system" namespace.
        # The empty string "" can be used to select non-namespaced resources.
        namespaces: ["kube-system"]
      # Log configmap and secret changes in all other namespaces at the Metadata level.
      - level: Metadata
        resources:
        - group: "" # core API group
          resources: ["secrets", "configmaps"]
      # Log all other resources in core and extensions at the Request level.
      - level: Request
        resources:
        - group: "" # core API group
        - group: "extensions" # Version of group should NOT be included.
      # A catch-all rule to log all other requests at the Metadata level.
      - level: Metadata
        # Long-running requests like watches that fall under this rule will not
        # generate an audit event in RequestReceived.
        omitStages:
          - "RequestReceived"
    EOF

    4.4.3 创建apiserver启动脚本

    cat >/opt/kubernetes/server/bin/kube-apiserver.sh <<'EOF'
    #!/bin/bash
    ./kube-apiserver \
      --apiserver-count 2 \
      --audit-log-path /data/logs/kubernetes/kube-apiserver/audit-log \
      --audit-policy-file ../conf/audit.yaml \
      --authorization-mode RBAC \
      --client-ca-file ./cert/ca.pem \
      --requestheader-client-ca-file ./cert/ca.pem \
      --enable-admission-plugins NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota \
      --etcd-cafile ./cert/ca.pem \
      --etcd-certfile ./cert/client.pem \
      --etcd-keyfile ./cert/client-key.pem \
      --etcd-servers https://172.26.216.164:2379,https://172.26.216.163:2379,https://172.26.216.161:2379 \
      --service-account-key-file ./cert/ca-key.pem \
      --service-cluster-ip-range 192.168.0.0/16 \
      --service-node-port-range 30000-32767 \
      --target-ram-mb=1024 \
      --kubelet-client-certificate ./cert/client.pem \
      --kubelet-client-key ./cert/client-key.pem \
      --log-dir  /data/logs/kubernetes/kube-apiserver \
      --tls-cert-file ./cert/apiserver.pem \
      --tls-private-key-file ./cert/apiserver-key.pem \
      --v 2
    EOF
    chmod +x /opt/kubernetes/server/bin/kube-apiserver.sh

    nohup sh ./kube-apiserver.sh >> test.log 2>&1 &


    部署启动所有apiserver机器
    集群其他机器的部署,没有不同的地方,所以略

10.配置反向代理两个apiServer,
    视频里面有用keepAlive做高可用，由于我这里用的阿里云,不做高可用，直接单机反代

    stream{
        upstream kube-apiserver{
            server 172.26.216.163:6433      max_fails=3 fail_timeout=30s;
            server 172.26.216.164:6433      max_fails=3 fail_timeout=30s;
        }
        server{
            listen 7443;
            proxy_connect_timeout 2s;
            proxy_timeout 900s;
            proxy_pass kube-apiserver;
        }
    }

    注意这里要配置在http外面


11.controller-manager 和 scheduler

    部署controller-manager服务
    apiserve、controller-manager、kube-scheduler三个服务所需的软件在同一套压缩包里面的，因此后两个服务不需要在单独解包
    而且这三个服务是在同一个主机上，互相之间通过http://127.0.0.1,也不需要证书

    4.5.1 创建controller-manager启动脚本

    cat >/opt/kubernetes/server/bin/kube-controller-manager.sh <<'EOF'
    #!/bin/sh
    ./kube-controller-manager \
      --cluster-cidr 172.7.0.0/16 \
      --leader-elect true \
      --log-dir /data/logs/kubernetes/kube-controller-manager \
      --master http://127.0.0.1:8080 \
      --service-account-private-key-file ./cert/ca-key.pem \
      --service-cluster-ip-range 192.168.0.0/16 \
      --root-ca-file ./cert/ca.pem \
      --v 2
    EOF
    # 授权
    chmod +x /opt/kubernetes/server/bin/kube-controller-manager.sh

    nohup sh ./kube-controller-manager.sh >> test2.log 2>&1 &

    4.6 部署kube-scheduler服务

    cat >/opt/kubernetes/server/bin/kube-scheduler.sh <<'EOF'
    #!/bin/sh
    ./kube-scheduler \
      --leader-elect  \
      --log-dir /data/logs/kubernetes/kube-scheduler \
      --master http://127.0.0.1:8080 \
      --v 2
    EOF
    # 授权
    chmod +x  /opt/kubernetes/server/bin/kube-scheduler.sh

     nohup sh ./kube-scheduler.sh >> test3.log 2>&1 &


    执行命令查看是否ok

    [root@iZ8vbijyckskmvy6mpnognZ bin]# ./kubectl get cs
    NAME                 STATUS    MESSAGE             ERROR
    controller-manager   Healthy   ok
    scheduler            Healthy   ok
    etcd-0               Healthy   {"health":"true"}
    etcd-1               Healthy   {"health":"true"}
    etcd-2               Healthy   {"health":"true"}




12.部署node节点
    签发kubelet证书

    cd /opt/certs/
    cat >/opt/certs/kubelet-csr.json <<EOF
    {
        "CN": "k8s-kubelet",
        "hosts": [
        "127.0.0.1",
        "172.26.216.164",
        "172.26.216.163",
        "172.26.216.162",
        "172.26.216.161",
        "172.26.216.142"
        ],
        "key": {
            "algo": "rsa",
            "size": 2048
        },
        "names": [
            {
                "C": "CN",
                "ST": "beijing",
                "L": "beijing",
                "O": "zq",
                "OU": "ops"
            }
        ]
    }
    EOF

    cfssl gencert \
          -ca=ca.pem \
          -ca-key=ca-key.pem \
          -config=ca-config.json \
          -profile=server \
          kubelet-csr.json | cfssljson -bare kubelet

    6.2 创建kubelet服务
    6.2.1 拷贝证书至node节点
        cd /opt/kubernetes/server/bin/cert
        scp hdss7-200:/opt/certs/kubelet.pem .
        scp hdss7-200:/opt/certs/kubelet-key.pem .


    6.2.2 创建kubelet配置
        创建kubelet的配置文件kubelet.kubeconfig比较麻烦,需要四步操作才能完成
        (1) set-cluster(设置集群参数)
        使用ca证书创建集群myk8s,使用的apiserver信息是10.4.7.10这个VIP
        cd /opt/kubernetes/server/conf/
        kubectl config set-cluster myk8s \
            --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \
            --embed-certs=true \
            --server=https://server.phantom5702.com:7443 \
            --kubeconfig=kubelet.kubeconfig
        (2) set-credentials(设置客户端认证参数)
        使用client证书创建用户k8s-node
        kubectl config set-credentials k8s-node \
            --client-certificate=/opt/kubernetes/server/bin/cert/client.pem \
            --client-key=/opt/kubernetes/server/bin/cert/client-key.pem \
            --embed-certs=true \
            --kubeconfig=kubelet.kubeconfig
        (3) set-context(绑定namespace)
        创建myk8s-context,关联集群myk8s和用户k8s-node
        kubectl config set-context myk8s-context \
            --cluster=myk8s \
            --user=k8s-node \
            --kubeconfig=kubelet.kubeconfig
        (4) use-context
        使用生成的配置文件向apiserver注册,注册信息会写入etcd,所以只需要注册一次即可
        kubectl config use-context myk8s-context --kubeconfig=kubelet.kubeconfig
        (5) 查看生成的kubelet.kubeconfig
        [root@hdss7-21 conf]# cat kubelet.kubeconfig
        apiVersion: v1
        clusters:
        - cluster:
            certificate-authority-data: xxxxxxxx
            server: https://10.4.7.10:7443
          name: myk8s
        contexts:
        - context:
            cluster: myk8s
            user: k8s-node
          name: myk8s-context
        current-context: myk8s-context
        kind: Config
        preferences: {}
        users:
        - name: k8s-node
          user:
            client-certificate-data: xxxxxxxx
            client-key-data: xxxxxxxx
        可以看出来,这个配置文件里面包含了集群名字,用户名字,集群认证的公钥,用户的公私钥等

    6.2.3 创建k8s-node.yaml配置文件
        cat >/opt/kubernetes/server/conf/k8s-node.yaml <<EOF
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: k8s-node
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:node
        subjects:
        - apiGroup: rbac.authorization.k8s.io
          kind: User
          name: k8s-node
        EOF
        使用RBAC鉴权规则,创建了一个ClusterRoleBinding的资源
        此资源中定义了一个user叫k8s-node
        给k8s-node用户绑定了角色ClusterRole,角色名为system:node
        使这个用户具有成为集群运算节点角色的权限
        由于这个用户名,同时也是kubeconfig中指定的用户,
        所以通过kubeconfig配置启动的kubelet节点,就能够成为node节点
    6.2.4 应用资源配置
        应用资源配置,并查看结果
        # 应用资源配置
        kubectl create -f /opt/kubernetes/server/conf/k8s-node.yaml
        # 查看集群角色和角色属性
        [root@hdss7-21 conf]# kubectl get clusterrolebinding k8s-node
        NAME       AGE
        k8s-node   13s
        [root@hdss7-21 conf]# kubectl get clusterrolebinding k8s-node -o yaml
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          creationTimestamp: "2020-04-22T14:38:09Z"
          name: k8s-node
          resourceVersion: "21217"
          selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/k8s-node
          uid: 597ffb0f-f92d-4eb5-aca2-2fe73397e2e4
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:node
        subjects:
        - apiGroup: rbac.authorization.k8s.io
          kind: User
          name: k8s-node

        #此时只是创建了相应的资源,还没有具体的node,如下验证
        [root@hdss7-21 conf]# kubectl get nodes
        No resources found.
    6.2.5 创建kubelet启动脚本
        --hostname-override参数每个node节点都不一样,是节点的主机名,注意修改
        cat >/opt/kubernetes/server/bin/kubelet.sh <<'EOF'
        #!/bin/sh
        ./kubelet \
          --hostname-override node164 \
          --anonymous-auth=false \
          --cgroup-driver systemd \
          --cluster-dns 192.168.0.2 \
          --cluster-domain cluster.local \
          --runtime-cgroups=/systemd/system.slice \
          --kubelet-cgroups=/systemd/system.slice \
          --fail-swap-on="false" \
          --client-ca-file ./cert/ca.pem \
          --tls-cert-file ./cert/kubelet.pem \
          --tls-private-key-file ./cert/kubelet-key.pem \
          --image-gc-high-threshold 20 \
          --image-gc-low-threshold 10 \
          --kubeconfig ../conf/kubelet.kubeconfig \
          --log-dir /data/logs/kubernetes/kube-kubelet \
          --pod-infra-container-image harbor.zq.com/public/pause:latest \
          --root-dir /data/kubelet
        EOF
    这里用没有指定dns的命令，因为我没有配置本地dns，试一下
    cat >/opt/kubernetes/server/bin/kubelet.sh <<'EOF'
    #!/bin/sh
    ./kubelet \
      --hostname-override 172.26.216.163 \
      --anonymous-auth=false \
      --cgroup-driver systemd \
      --cluster-dns 192.168.0.2 \
      --cluster-domain cluster.local \
      --runtime-cgroups=/systemd/system.slice \
      --kubelet-cgroups=/systemd/system.slice \
      --fail-swap-on="false" \
      --client-ca-file ./cert/ca.pem \
      --tls-cert-file ./cert/kubelet.pem \
      --tls-private-key-file ./cert/kubelet-key.pem \
      --image-gc-high-threshold 20 \
      --image-gc-low-threshold 10 \
      --kubeconfig ../conf/kubelet.kubeconfig \
      --log-dir /data/logs/kubernetes/kube-kubelet \
      --pod-infra-container-image harbor.phantom5702.com/public/pause:latest \
      --root-dir /data/kubelet
    EOF

    # 创建目录&授权
    chmod +x /opt/kubernetes/server/bin/kubelet.sh
    mkdir -p /data/logs/kubernetes/kube-kubelet
    mkdir -p /data/kubelet

    没有kubelet.kubeconfig 记得复制一下
    nohup sh ./kubelet.sh >> test10.log 2>&1 &


    由于没有配置内部dns，被迫使用ip地址，以后这里可以配置一下bind9
    kubectl get nodes
    NAME             STATUS   ROLES    AGE     VERSION
    172.26.216.163   Ready    <none>   5m27s   v1.15.11
    172.26.216.164   Ready    <none>   8m7s    v1.15.11

    6.2.8 部署其他node节点
        第一个节点部署完成后,其他节点就要简单很多,只需拷贝kubelet.kubeconfig配置到本地后,创建启动脚本并用`supervisord启动即可
        也可以不拷贝配置文件,就需要手动再执行创建配置文件的四步
        # 拷贝证书
        cd /opt/kubernetes/server/bin/cert
        scp hdss7-200:/opt/certs/kubelet.pem .
        scp hdss7-200:/opt/certs/kubelet-key.pem .
        # 拷贝配置文件
        cd /opt/kubernetes/server/conf/
        scp hdss7-21:/opt/kubernetes/server/conf/kubelet.kubeconfig .
        拷贝完配置后,剩下的步骤参考6.2.5 创建kubelet启动脚本,除脚本中--hostname-override不同外,其他都一样
    6.2.9 检查所有节点并给节点打上标签
        此操作非必须,因为只是打的一个标签,方便识别而已
        kubectl get nodes
        kubectl label node hdss7-21.host.com node-role.kubernetes.io/master=
        kubectl label node hdss7-21.host.com node-role.kubernetes.io/node=
        [root@hdss7-22 cert]# kubectl get nodes
        NAME                STATUS   ROLES         AGE   VERSION
        hdss7-21.host.com   Ready    master,node   9m    v1.15.5
        hdss7-22.host.com   Ready    <none>        64s   v1.15.5

        kubectl label node 172.26.216.163  node-role.kubernetes.io/master=
        kubectl label node 172.26.216.163  node-role.kubernetes.io/node=
        kubectl label node 172.26.216.164  node-role.kubernetes.io/node=
    6.3 创建kube-proxy服务
       签发证书在7.200上
    6.3.1 签发kube-proxy证书
        (1) 创建生成证书csr的json配置文件

        cd /opt/certs/
        cat >/opt/certs/kube-proxy-csr.json <<EOF
        {
            "CN": "system:kube-proxy",
            "key": {
                "algo": "rsa",
                "size": 2048
            },
            "names": [
                {
                    "C": "CN",
                    "ST": "beijing",
                    "L": "beijing",
                    "O": "zq",
                    "OU": "ops"
                }
            ]
        }
        EOF

        cfssl gencert \
              -ca=ca.pem \
              -ca-key=ca-key.pem \
              -config=ca-config.json \
              -profile=client \
              kube-proxy-csr.json |cfssljson -bare kube-proxy-client


        拷贝证书至node节点
        cd /opt/kubernetes/server/bin/cert
        scp hdss7-200:/opt/certs/kube-proxy-client.pem .
        scp hdss7-200:/opt/certs/kube-proxy-client-key.pem .

        6.3.3 创建kube-proxy配置
        同样是四步操作,类似kubelet

        同样是四步操作,类似kubelet
        (1) set-cluster
        cd /opt/kubernetes/server/conf/
        kubectl config set-cluster myk8s \
            --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \
            --embed-certs=true \
            --server=https://server.phantom5702.com:7443 \
            --kubeconfig=kube-proxy.kubeconfig
        (2) set-credentials
        kubectl config set-credentials kube-proxy \
            --client-certificate=/opt/kubernetes/server/bin/cert/kube-proxy-client.pem \
            --client-key=/opt/kubernetes/server/bin/cert/kube-proxy-client-key.pem \
            --embed-certs=true \
            --kubeconfig=kube-proxy.kubeconfig
        (3) set-context
        kubectl config set-context myk8s-context \
            --cluster=myk8s \
            --user=kube-proxy \
            --kubeconfig=kube-proxy.kubeconfig
        (4) use-context
        kubectl config use-context myk8s-context --kubeconfig=kube-proxy.kubeconfig



        6.3.4 加载ipvs模块以备kube-proxy启动用
        # 创建开机ipvs脚本
        cat >/etc/ipvs.sh <<'EOF'
        #!/bin/bash
        ipvs_mods_dir="/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs"
        for i in $(ls $ipvs_mods_dir|grep -o "^[^.]*")
        do
          /sbin/modinfo -F filename $i &>/dev/null
          if [ $? -eq 0 ];then
            /sbin/modprobe $i
          fi
        done
        EOF
        # 执行脚本开启ipvs
        sh /etc/ipvs.sh
        # 验证开启结果
        [root@hdss7-21 conf]# lsmod |grep ip_vs
        ip_vs_wrr              12697  0
        ip_vs_wlc              12519  0
        ......略

        6.3.5 创建kube-proxy启动脚本
        同上,--hostname-override参数在不同的node节点上不一样,需修改
        cat >/opt/kubernetes/server/bin/kube-proxy.sh <<'EOF'
        #!/bin/sh
        ./kube-proxy \
          --hostname-override 172.26.216.164 \
          --cluster-cidr 172.7.0.0/16 \
          --proxy-mode=ipvs \
          --ipvs-scheduler=nq \
          --kubeconfig ../conf/kube-proxy.kubeconfig
        EOF
        # 授权
        chmod +x /opt/kubernetes/server/bin/kube-proxy.sh

        6.3.7 启动服务并检查
        mkdir -p /data/logs/kubernetes/kube-proxy
        nohup sh ./kube-proxy.sh >> test11.log 2>&1 &

        [root@hdss7-21 conf]# kubectl get svc
        NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
        kubernetes   ClusterIP   192.168.0.1   <none>        443/TCP   47h
        # 检查ipvs,是否新增了配置
        yum install ipvsadm -y

        [root@iZ8vbijyckskmvy6mpnogmZ bin]# ipvsadm -Ln
        IP Virtual Server version 1.2.1 (size=4096)
        Prot LocalAddress:Port Scheduler Flags
          -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
        TCP  192.168.0.1:443 nq
          -> 172.26.216.163:6443          Masq    1      0          0
          -> 172.26.216.164:6443          Masq    1      0          0



        6.3.8 部署所有节点
        首先需拷贝kube-proxy.kubeconfig 到 hdss7-22.host.com的conf目录下
        # 拷贝证书文件
        cd /opt/kubernetes/server/bin/cert
        scp hdss7-200:/opt/certs/kube-proxy-client.pem .
        scp hdss7-200:/opt/certs/kube-proxy-client-key.pem .
        # 拷贝配置文件
        cd /opt/kubernetes/server/conf/
        scp hdss7-21:/opt/kubernetes/server/conf/kube-proxy.kubeconfig .

        7 验证kubernetes集群
        7.1 在任意一个节点上创建一个资源配置清单
        cat >/root/nginx-ds.yaml <<'EOF'
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: nginx-ds
        spec:
          template:
            metadata:
              labels:
                app: nginx-ds
            spec:
              containers:
              - name: my-nginx
                image: harbor.phantom5702.com/public/nginx:1.7.9
                ports:
                - containerPort: 80
                - hostPort: 5002
        EOF

                kubectl create -f /root/nginx-ds.yaml
                kubectl get pods
                kubectl delete -f /root/nginx-ds.yaml

        7.2.3 查看kubernetes是否搭建好
        [root@hdss7-22 conf]# kubectl get cs
        NAME                 STATUS    MESSAGE              ERROR
        etcd-0               Healthy   {"health": "true"}
        etcd-2               Healthy   {"health": "true"}
        etcd-1               Healthy   {"health": "true"}
        controller-manager   Healthy   ok
        scheduler            Healthy   ok
        [root@hdss7-21 ~]# kubectl get nodes
        NAME                STATUS   ROLES         AGE    VERSION
        hdss7-21.host.com   Ready    master,node   6d1h   v1.15.5
        hdss7-22.host.com   Ready    <none>        6d1h   v1.15.5
        [root@hdss7-22 ~]# kubectl get pods
        NAME             READY   STATUS    RESTARTS   AGE
        nginx-ds-j777c   1/1     Running   0          6m45s
        nginx-ds-nwsd6   1/1     Running   0          6m45s


